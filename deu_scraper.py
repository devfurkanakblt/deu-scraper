#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DEÃœ Web Scraper
Bu script belirtilen DEÃœ sayfalarÄ±ndan rel='bookmark' olan a etiketlerindeki linkleri Ã§eker.
"""

import requests
from bs4 import BeautifulSoup
import time
import json
from urllib.parse import urljoin, urlparse
import sys
import os
from dotenv import load_dotenv

class DEUScraper:
    def __init__(self):
        # .env dosyasÄ±nÄ± yÃ¼kle
        load_dotenv()
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Pushbullet API anahtarÄ±
        self.pushbullet_api_key = os.getenv('PUSHBULLET_API_KEY')
        if not self.pushbullet_api_key:
            print("âš ï¸  UYARI: PUSHBULLET_API_KEY .env dosyasÄ±nda bulunamadÄ±!")
            print("   Pushbullet bildirimleri gÃ¶nderilmeyecek.")
            print("   .env dosyasÄ± oluÅŸturun ve PUSHBULLET_API_KEY=your_key_here ekleyin")
        
    def get_page_content(self, url):
        """
        Verilen URL'den sayfa iÃ§eriÄŸini Ã§eker
        """
        try:
            print(f"Sayfa yÃ¼kleniyor: {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Hata: {url} yÃ¼klenirken hata oluÅŸtu: {e}")
            return None
    
    def extract_bookmark_links(self, html_content, base_url, limit=5):
        """
        HTML iÃ§eriÄŸinden rel='bookmark' olan a etiketlerindeki linkleri Ã§Ä±karÄ±r
        Sadece ilk 'limit' kadar link alÄ±r (varsayÄ±lan: 5)
        """
        if not html_content:
            return []
            
        soup = BeautifulSoup(html_content, 'html.parser')
        bookmark_links = []
        
        # rel='bookmark' olan a etiketlerini bul
        bookmark_elements = soup.find_all('a', rel='bookmark')
        
        # Sadece ilk 'limit' kadar link al
        for element in bookmark_elements[:limit]:
            href = element.get('href')
            if href:
                # GÃ¶receli URL'leri mutlak URL'ye Ã§evir
                absolute_url = urljoin(base_url, href)
                link_text = element.get_text(strip=True)
                
                bookmark_links.append({
                    'url': absolute_url,
                    'text': link_text,
                    'title': element.get('title', ''),
                    'base_url': base_url
                })
        
        return bookmark_links
    
    def scrape_url(self, url):
        """
        Tek bir URL'yi scrape eder
        """
        print(f"\n{'='*60}")
        print(f"Scraping: {url}")
        print(f"{'='*60}")
        
        html_content = self.get_page_content(url)
        if html_content:
            bookmark_links = self.extract_bookmark_links(html_content, url)
            print(f"Bulunan bookmark link sayÄ±sÄ±: {len(bookmark_links)}")
            
            for i, link in enumerate(bookmark_links, 1):
                print(f"{i:2d}. {link['text']}")
                print(f"    URL: {link['url']}")
                if link['title']:
                    print(f"    Title: {link['title']}")
                print()
            
            return bookmark_links
        else:
            print(f"Sayfa yÃ¼klenemedi: {url}")
            return []
    
    def scrape_all_urls(self, urls):
        """
        TÃ¼m URL'leri scrape eder
        """
        all_links = []
        
        for url in urls:
            links = self.scrape_url(url)
            all_links.extend(links)
            
            # Rate limiting - sayfalar arasÄ± bekleme
            time.sleep(2)
        
        return all_links
    
    def load_existing_bookmarks(self, filename='deu_bookmark_links.json'):
        """
        Mevcut JSON dosyasÄ±ndan bookmarklarÄ± yÃ¼kler
        """
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            print(f"{filename} dosyasÄ± bulunamadÄ±. Yeni dosya oluÅŸturulacak.")
            return []
        except Exception as e:
            print(f"Mevcut dosya okunurken hata oluÅŸtu: {e}")
            return []
    
    def find_new_bookmarks(self, new_links, existing_links):
        """
        Yeni linkler ile mevcut linkleri karÅŸÄ±laÅŸtÄ±rÄ±r ve sadece yeni olanlarÄ± dÃ¶ndÃ¼rÃ¼r
        """
        existing_urls = {link['url'] for link in existing_links}
        new_bookmarks = []
        
        for link in new_links:
            if link['url'] not in existing_urls:
                new_bookmarks.append(link)
        
        return new_bookmarks
    
    def send_pushbullet_notification(self, title, body, url=None):
        """
        Pushbullet API kullanarak bildirim gÃ¶nderir
        """
        if not self.pushbullet_api_key:
            print(f"ðŸ“± Pushbullet bildirimi gÃ¶nderilemedi (API anahtarÄ± yok): {title}")
            return False
        
        try:
            pushbullet_url = "https://api.pushbullet.com/v2/pushes"
            headers = {
                'Access-Token': self.pushbullet_api_key,
                'Content-Type': 'application/json'
            }
            
            data = {
                'type': 'note',
                'title': title,
                'body': body
            }
            
            # URL varsa link olarak ekle
            if url:
                data['type'] = 'link'
                data['url'] = url
            
            response = requests.post(pushbullet_url, headers=headers, json=data)
            response.raise_for_status()
            
            print(f"ðŸ“± Pushbullet bildirimi gÃ¶nderildi: {title}")
            return True
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ Pushbullet bildirimi gÃ¶nderilemedi: {e}")
            return False
        except Exception as e:
            print(f"âŒ Pushbullet hatasÄ±: {e}")
            return False
    
    def send_new_bookmark_notifications(self, new_bookmarks):
        """
        Yeni bookmarklar iÃ§in Pushbullet bildirimleri gÃ¶nderir
        """
        if not new_bookmarks:
            return
        
        print(f"\nðŸ“± {len(new_bookmarks)} yeni bookmark iÃ§in Pushbullet bildirimi gÃ¶nderiliyor...")
        
        for bookmark in new_bookmarks:
            title = "ðŸ”– Yeni DEÃœ Duyurusu"
            body = f"{bookmark['text']}\n\nKaynak: {bookmark['base_url']}"
            url = bookmark['url']
            
            self.send_pushbullet_notification(title, body, url)
            
            # Rate limiting - API limitlerini aÅŸmamak iÃ§in
            time.sleep(1)
    
    def save_results(self, results, filename='deu_bookmark_links.json'):
        """
        SonuÃ§larÄ± JSON dosyasÄ±na kaydeder
        """
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"\nSonuÃ§lar {filename} dosyasÄ±na kaydedildi.")
        except Exception as e:
            print(f"SonuÃ§lar kaydedilirken hata oluÅŸtu: {e}")
    
    def append_new_bookmarks(self, new_bookmarks, filename='deu_bookmark_links.json'):
        """
        Yeni bookmarklarÄ± mevcut dosyaya ekler
        """
        try:
            # Mevcut bookmarklarÄ± yÃ¼kle
            existing_bookmarks = self.load_existing_bookmarks(filename)
            
            # Yeni bookmarklarÄ± ekle
            all_bookmarks = existing_bookmarks + new_bookmarks
            
            # Dosyaya kaydet
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(all_bookmarks, f, ensure_ascii=False, indent=2)
            
            print(f"\n{len(new_bookmarks)} yeni bookmark {filename} dosyasÄ±na eklendi.")
            print(f"Toplam bookmark sayÄ±sÄ±: {len(all_bookmarks)}")
            
        except Exception as e:
            print(f"Yeni bookmarklar eklenirken hata oluÅŸtu: {e}")
    
    def print_summary(self, results):
        """
        SonuÃ§larÄ±n Ã¶zetini yazdÄ±rÄ±r
        """
        print(f"\n{'='*60}")
        print("Ã–ZET")
        print(f"{'='*60}")
        print(f"Toplam bookmark link sayÄ±sÄ±: {len(results)}")
        
        # URL'lere gÃ¶re grupla
        url_groups = {}
        for link in results:
            base_url = link['base_url']
            if base_url not in url_groups:
                url_groups[base_url] = []
            url_groups[base_url].append(link)
        
        for base_url, links in url_groups.items():
            print(f"\n{base_url}: {len(links)} link")
            for link in links[:3]:  # Ä°lk 3 linki gÃ¶ster
                print(f"  - {link['text']}")
            if len(links) > 3:
                print(f"  ... ve {len(links) - 3} link daha")

def main():
    """
    Ana fonksiyon
    """
    # Scrape edilecek URL'ler
    urls = [
        "https://www.deu.edu.tr/tum-duyurular/",
        "https://csc.deu.edu.tr/tr/",
        "https://fen.deu.edu.tr/tr/"
    ]
    
    print("DEÃœ Web Scraper BaÅŸlatÄ±lÄ±yor...")
    print("Bu script belirtilen sayfalardan rel='bookmark' olan linkleri Ã§ekecek.")
    print("Her sayfadan sadece ilk 5 link alÄ±nacak ve sadece yeni bookmarklar eklenecek.")
    
    scraper = DEUScraper()
    
    try:
        # Mevcut bookmarklarÄ± yÃ¼kle
        existing_bookmarks = scraper.load_existing_bookmarks()
        print(f"\nMevcut bookmark sayÄ±sÄ±: {len(existing_bookmarks)}")
        
        # TÃ¼m URL'leri scrape et (her sayfadan sadece 5 link)
        new_results = scraper.scrape_all_urls(urls)
        
        # Yeni bookmarklarÄ± bul
        new_bookmarks = scraper.find_new_bookmarks(new_results, existing_bookmarks)
        
        print(f"\nBu kontrolde bulunan toplam link: {len(new_results)}")
        print(f"Yeni bookmark sayÄ±sÄ±: {len(new_bookmarks)}")
        
        if new_bookmarks:
            # Sadece yeni bookmarklarÄ± ekle
            scraper.append_new_bookmarks(new_bookmarks)
            
            # Pushbullet bildirimleri gÃ¶nder
            scraper.send_new_bookmark_notifications(new_bookmarks)
            
            # Yeni bookmarklarÄ± gÃ¶ster
            print(f"\n{'='*60}")
            print("YENÄ° BULUNAN BOOKMARKLAR")
            print(f"{'='*60}")
            for i, bookmark in enumerate(new_bookmarks, 1):
                print(f"{i:2d}. {bookmark['text']}")
                print(f"    URL: {bookmark['url']}")
                print(f"    Kaynak: {bookmark['base_url']}")
                print()
        else:
            print("\nYeni bookmark bulunamadÄ±. TÃ¼m linkler zaten mevcut.")
        
        # GÃ¼ncellenmiÅŸ Ã¶zet
        updated_bookmarks = scraper.load_existing_bookmarks()
        scraper.print_summary(updated_bookmarks)
        
        print(f"\nScraping tamamlandÄ±! Toplam {len(updated_bookmarks)} bookmark mevcut.")
        
    except KeyboardInterrupt:
        print("\nScraping kullanÄ±cÄ± tarafÄ±ndan durduruldu.")
    except Exception as e:
        print(f"\nBeklenmeyen hata: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
